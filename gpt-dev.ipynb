{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input text file\n",
    "with open(\"data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Number of unique characters:  65\n"
     ]
    }
   ],
   "source": [
    "# All the unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Number of unique characters: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "# Here we build a character-level model so we need to convert each character to a unique integer.\n",
    "\n",
    "# Create a mapping from characters to integers - dictionaries\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # stoi: string to integer\n",
    "itos = { i:ch for i,ch in enumerate(chars) } # itos: integer to string\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Test the encoder and decoder\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))\n",
    "\n",
    "# There are different schemas/methods to tokenize textual data -> usually subword tokenization is used for word-level models\n",
    "# But we keep it simple here and use character-level tokenization for easier understanding and implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56,  ..., 45,  8,  0]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Now, encode the entire text \n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # Tensor is a generalized form of a matrix that can have any number of dimensions\n",
    "print(data, data.dtype)\n",
    "print(data[:1000]) # This is what the first 1000 characters from before will look like to the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "n = int(0.9 * len(data)) # first 90% for training\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usualy we do not train on entire dataset at once, but in batches -> block_size, context_size etc.\n",
    "block_size = 8\n",
    "train_data[:block_size+1] # first 9 characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) then target is 47\n",
      "When input is tensor([18, 47]) then target is 56\n",
      "When input is tensor([18, 47, 56]) then target is 57\n",
      "When input is tensor([18, 47, 56, 57]) then target is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) then target is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) then target is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) then target is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then target is 58\n"
     ]
    }
   ],
   "source": [
    "# Chunk of 9 characters has 8 examples (see below)\n",
    "# We train on 8 examples with context size between 1 and block_size (8 in this case)\n",
    "# We want to transformer to know every context size from 1 to up to block_size\n",
    "# Transformer will never receive more than block_size context size at once \n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} then target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "When input is [24] then target is 43\n",
      "When input is [24, 43] then target is 58\n",
      "When input is [24, 43, 58] then target is 5\n",
      "When input is [24, 43, 58, 5] then target is 57\n",
      "When input is [24, 43, 58, 5, 57] then target is 1\n",
      "When input is [24, 43, 58, 5, 57, 1] then target is 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] then target is 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] then target is 39\n",
      "When input is [44] then target is 53\n",
      "When input is [44, 53] then target is 56\n",
      "When input is [44, 53, 56] then target is 1\n",
      "When input is [44, 53, 56, 1] then target is 58\n",
      "When input is [44, 53, 56, 1, 58] then target is 46\n",
      "When input is [44, 53, 56, 1, 58, 46] then target is 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] then target is 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] then target is 1\n",
      "When input is [52] then target is 58\n",
      "When input is [52, 58] then target is 1\n",
      "When input is [52, 58, 1] then target is 58\n",
      "When input is [52, 58, 1, 58] then target is 46\n",
      "When input is [52, 58, 1, 58, 46] then target is 39\n",
      "When input is [52, 58, 1, 58, 46, 39] then target is 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] then target is 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] then target is 46\n",
      "When input is [25] then target is 17\n",
      "When input is [25, 17] then target is 27\n",
      "When input is [25, 17, 27] then target is 10\n",
      "When input is [25, 17, 27, 10] then target is 0\n",
      "When input is [25, 17, 27, 10, 0] then target is 21\n",
      "When input is [25, 17, 27, 10, 0, 21] then target is 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] then target is 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] then target is 39\n"
     ]
    }
   ],
   "source": [
    "# GPUs work in parallel so we would like to use parallelizable \"chunks\"\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # How many independent sequences will we process in parallel?\n",
    "block_size = 8 # What is the maximum context length for prediction?\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # random starting indices for the sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # input sequences, stacked as rows in a tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # target sequences, shifted one position to the right, stacked as rows in a tensor\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# This means 32 examples in both x and y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('Inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('Targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()} then target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Does the Bigram Language Model Do?\n",
    "\n",
    "- The Bigram Language Model predicts the next token in a sequence based on the current token. For each token in the input sequence, the model looks up a vector from the embedding table, which represents the likelihood of each possible next token (vocabulary size).\n",
    "- In essence, it's learning bigram statistics—the probability of one token following another. It doesn't use sophisticated context like more advanced models, such as GPT, which consider the entire sequence of previous tokens.\n",
    "\n",
    "This simple model serves as a foundation for more complex language models. It directly uses the current token to predict the next token without considering any longer context (like trigrams or beyond)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # Essentially, this layer is a lookup table that outputs a vector (of size vocab_size) for each token in the input. \n",
    "        # In this case, it's used to predict the next token (bigram prediction).\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both [batch_size, block_size] tensors of integers\n",
    "        # The output is a tensor of shape [batch_size, block_size, vocab_size], where each value in the vocab_size dimension represents the score (logit) for a possible next token.\n",
    "        # This means for each token in the input, the model produces a vector of vocab_size logits that represent the predicted probabilities of the next token in the sequence (the bigram prediction).\n",
    "        logits = self.token_embedding_table(idx) # [batch_size, block_size, vocab_size] or (B, T, C)\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:   \n",
    "            # Inuitively, we want to predict the next token in the sequence, given the current token.\n",
    "            # We can do this by comparing the predicted logits to the actual next token in the sequence.\n",
    "            # This is done using the cross-entropy loss, which measures the difference between the predicted probabilities and the actual target (the next token in the sequence).\n",
    "            \n",
    "            # Cross entropy wants a (B, C, T) input, so we need to permute the logits tensor\n",
    "            # It's mainly important that C is in second position so we just make 2-dimensional tensor like below\n",
    "            # Basically stretching the array\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # This generate function does not make much sense for the Bigram model because Bigram only uses one character to predict the next\n",
    "    # But we would like to keep it fixed to use with other more complex models\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) tensor of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions for the next token\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # Becomes (B, C) as we remove the time dimension\n",
    "\n",
    "            # apply softmax to convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "            # sample from the probability distribution to get the next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # append index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# We would expect a loss of -ln(1/vocab_size) = -ln(1/65) = 4.17\n",
    "\n",
    "# generate some new text from the model\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # 1 x 1 tensor with a zero to start with (new line character)\n",
    "\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) # index into the first batch and convert to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "# can get away with a kind of high learning rate because the model is so simple\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.390815258026123\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tod on resheen doffousatheonisuckiceyoul me ee\n",
      "T:\n",
      "PShintherd fu h ta w\n",
      "\n",
      "COMende pos pitl tot s maru loofrlomson En; by; me t?\n",
      "manchure.\n",
      "CERoof Ye, INCEShe h hatharvecowitofochand tee oucough s; he als'd:\n",
      "Alens iostist, t med thesthoa us, m s an;\n",
      "An ase\n",
      "\n",
      "heruerdeve-n o'sk;\n",
      "GAnouilerrlince hane s s \n"
     ]
    }
   ],
   "source": [
    "# Text of optimized bigram model\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist())) # index into the first batch and convert to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt_3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
